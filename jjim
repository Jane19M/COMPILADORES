import re
from collections import defaultdict
from graphviz import Digraph

# Lista de tokens con su respectiva expresión regular para identificar diferentes componentes del código
TOKENS = [
    ("CLAVES", r'\b(if|else|while|for|return|break|continue|def|class|print|int)\b'),  # Palabras clave de Python
    ("IDENTIFICADORES", r'\b[a-zA-Z_]\w*\b'),                                # Identificadores válidos
    ("NUMEROS", r'\b\d+(\.\d+)?\b'),                                         # Números enteros y decimales
    ("OPERADORES", r'[+\-*/%=<>!&|^~]'),                                     # Operadores aritméticos y lógicos
    ("STRING", r'"[^"\\]*(\\.[^"\\]*)*"'),                                   # Cadenas de texto con escapado
    ("SALTOS DE LINEA", r'\n'),                                              # Saltos de línea
    ("ESPACIOS", r'[ \t]+'),                                                 # Espacios y tabulaciones
    ("COMENTARIOS", r'#.*'),                                                 # Comentarios de línea
    ("DELIMITADORES", r'[(){}[\],.;:]'),                                     # Delimitadores como paréntesis y comas
]

def tokenize(code):
    """
    Función que toma un código fuente como entrada y devuelve los tokens identificados por categorías.

    Args:
        code (str): Código fuente a analizar.

    Returns:
        defaultdict(list): Diccionario con los tokens clasificados por categorías.
    """
    tokens = defaultdict(list)  # Diccionario que almacenará los tokens categorizados
    position = 0  # Posición actual en el código fuente

    # Recorre el código fuente mientras queden caracteres por analizar
    while position < len(code):
        match = None  # Variable para almacenar la coincidencia del token
        # Itera sobre las categorías de tokens
        for token_type, token_regex in TOKENS:
            regex = re.compile(token_regex)  # Compila la expresión regular actual
            match = regex.match(code, position)  # Busca coincidencias en la posición actual
            if match:
                # Si el token no es un espacio o un comentario, lo almacena en el diccionario
                if token_type not in ["ESPACIOS", "COMENTARIOS"]:
                    tokens[token_type].append(match.group(0))
                position = match.end(0)  # Actualiza la posición para seguir analizando
                break  # Si se encuentra un token, se sale del ciclo para evitar seguir probando

        # Si no se encuentra un token válido, reporta un error
        if not match:
            print(f"Error: Token desconocido en la posición {position}")
            break
    
    return tokens  # Devuelve los tokens encontrados y clasificados

def generate_syntax_tree_image(tokens, filename="syntax_tree"):
    """
    Genera un árbol sintáctico y lo guarda como imagen.

    Args:
        tokens (defaultdict(list)): Diccionario con los tokens clasificados por categorías.
        filename (str): Nombre del archivo de salida (sin extensión).
    """
    dot = Digraph(comment="Árbol Sintáctico")

    # Añade cada tipo de token como un nodo en el gráfico
    for token_type, token_values in tokens.items():
        token_node = token_type  # El tipo de token es el nodo principal
        dot.node(token_node, token_type)  # Crea un nodo para la categoría
        
        # Para cada token encontrado, se crea un subnodo
        for i, token in enumerate(token_values):
            token_value_node = f"{token_type}_{i}"
            # Escapar caracteres especiales en el token
            escaped_token = token.replace('"', '\\"')
            dot.node(token_value_node, escaped_token)
            dot.edge(token_node, token_value_node)  # Conecta el tipo de token con su valor

    # Guarda el gráfico en formato PNG
    dot.render(filename, format='png')
    print(f"Árbol sintáctico generado y guardado como {filename}.png")

# Solicita al usuario que ingrese el código a analizar
print("Por favor, ingrese el código fuente a analizar (deje una línea vacía para finalizar):")

lineas = []  # Lista para almacenar las líneas de código ingresadas por el usuario
# Bucle para permitir la entrada de múltiples líneas de código
while True:
    linea = input()  # Captura la entrada del usuario
    if linea == "":  # Si se deja una línea vacía, se finaliza la entrada
        break
    lineas.append(linea)  # Añade la línea a la lista

codigo_fuente = "\n".join(lineas)  # Une todas las líneas en una cadena de texto

# Llama a la función de tokenización y almacena el resultado
tokens = tokenize(codigo_fuente)

# Muestra los tokens identificados agrupados por categoría
print("\nTokens identificados agrupados por categoría:")
for token_type, token_values in tokens.items():
    print(f"{token_type}: {', '.join(token_values)}")

# Genera el árbol sintáctico como una imagen
generate_syntax_tree_image(tokens, "syntax_tree")



"""
CODIGO DE PRUEBA 

def semantic_analysis(tokens):
   
Función para realizar un análisis semántico básico en los tokens.
  
errors = []
identifiers = set()  # Conjunto para almacenar identificadores declarados
declared_identifiers = set()  # Conjunto para identificar qué variables se han declarado
result = "10" + 5

"""
